{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# roBERTo Classifier\n",
    "\n",
    "This notebook inspired in BERT/roBERTa models andd attaches an output layer that works as multiclassification. As well, this model is trained with a spanish corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"muchocine\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "print(f'Features: { dataset.features }. Instances: { dataset.num_rows }')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset muchocine (/Users/ernestomancebo/.cache/huggingface/datasets/muchocine/default/1.1.1/3ed5582584cd84ef722606a3d725ef18fd4647d63195fef05c47683e5a056ccd)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: {'review_body': Value(dtype='string', id=None), 'review_summary': Value(dtype='string', id=None), 'star_rating': Value(dtype='int32', id=None)}. Instances: 3872\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "source": [
    "summary_len = [len(str(x).split()) for x in dataset['review_summary']]\n",
    "body_len = [len(str(x).split()) for x in dataset['review_body']]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "sns.set_style('darkgrid')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "_ = sns.displot(summary_len, kde=True, height=8)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"564.539844pt\" version=\"1.1\" viewBox=\"0 0 565.430469 564.539844\" width=\"565.430469pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-30T18:50:27.158418</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 564.539844 \nL 565.430469 564.539844 \nL 565.430469 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.025469 537.78 \nL 558.230469 537.78 \nL 558.230469 7.2 \nL 50.025469 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 66.708966 537.78 \nL 66.708966 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(63.650451 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 266 2259 \nQ 266 3072 433 3567 \nQ 600 4063 929 4331 \nQ 1259 4600 1759 4600 \nQ 2128 4600 2406 4451 \nQ 2684 4303 2865 4023 \nQ 3047 3744 3150 3342 \nQ 3253 2941 3253 2259 \nQ 3253 1453 3087 958 \nQ 2922 463 2592 192 \nQ 2263 -78 1759 -78 \nQ 1097 -78 719 397 \nQ 266 969 266 2259 \nz\nM 844 2259 \nQ 844 1131 1108 757 \nQ 1372 384 1759 384 \nQ 2147 384 2411 759 \nQ 2675 1134 2675 2259 \nQ 2675 3391 2411 3762 \nQ 2147 4134 1753 4134 \nQ 1366 4134 1134 3806 \nQ 844 3388 844 2259 \nz\n\" id=\"ArialMT-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 130.876264 537.78 \nL 130.876264 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(124.759233 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 2384 0 \nL 1822 0 \nL 1822 3584 \nQ 1619 3391 1289 3197 \nQ 959 3003 697 2906 \nL 697 3450 \nQ 1169 3672 1522 3987 \nQ 1875 4303 2022 4600 \nL 2384 4600 \nL 2384 0 \nz\n\" id=\"ArialMT-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 195.043562 537.78 \nL 195.043562 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(188.926531 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 3222 541 \nL 3222 0 \nL 194 0 \nQ 188 203 259 391 \nQ 375 700 629 1000 \nQ 884 1300 1366 1694 \nQ 2113 2306 2375 2664 \nQ 2638 3022 2638 3341 \nQ 2638 3675 2398 3904 \nQ 2159 4134 1775 4134 \nQ 1369 4134 1125 3890 \nQ 881 3647 878 3216 \nL 300 3275 \nQ 359 3922 746 4261 \nQ 1134 4600 1788 4600 \nQ 2447 4600 2831 4234 \nQ 3216 3869 3216 3328 \nQ 3216 3053 3103 2787 \nQ 2991 2522 2730 2228 \nQ 2469 1934 1863 1422 \nQ 1356 997 1212 845 \nQ 1069 694 975 541 \nL 3222 541 \nz\n\" id=\"ArialMT-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 259.21086 537.78 \nL 259.21086 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g style=\"fill:#262626;\" transform=\"translate(253.093829 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 269 1209 \nL 831 1284 \nQ 928 806 1161 595 \nQ 1394 384 1728 384 \nQ 2125 384 2398 659 \nQ 2672 934 2672 1341 \nQ 2672 1728 2419 1979 \nQ 2166 2231 1775 2231 \nQ 1616 2231 1378 2169 \nL 1441 2663 \nQ 1497 2656 1531 2656 \nQ 1891 2656 2178 2843 \nQ 2466 3031 2466 3422 \nQ 2466 3731 2256 3934 \nQ 2047 4138 1716 4138 \nQ 1388 4138 1169 3931 \nQ 950 3725 888 3313 \nL 325 3413 \nQ 428 3978 793 4289 \nQ 1159 4600 1703 4600 \nQ 2078 4600 2393 4439 \nQ 2709 4278 2876 4000 \nQ 3044 3722 3044 3409 \nQ 3044 3113 2884 2869 \nQ 2725 2625 2413 2481 \nQ 2819 2388 3044 2092 \nQ 3269 1797 3269 1353 \nQ 3269 753 2831 336 \nQ 2394 -81 1725 -81 \nQ 1122 -81 723 278 \nQ 325 638 269 1209 \nz\n\" id=\"ArialMT-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-33\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 323.378158 537.78 \nL 323.378158 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(317.261127 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 2069 0 \nL 2069 1097 \nL 81 1097 \nL 81 1613 \nL 2172 4581 \nL 2631 4581 \nL 2631 1613 \nL 3250 1613 \nL 3250 1097 \nL 2631 1097 \nL 2631 0 \nL 2069 0 \nz\nM 2069 1613 \nL 2069 3678 \nL 634 1613 \nL 2069 1613 \nz\n\" id=\"ArialMT-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-34\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 387.545456 537.78 \nL 387.545456 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(381.428425 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 266 1200 \nL 856 1250 \nQ 922 819 1161 601 \nQ 1400 384 1738 384 \nQ 2144 384 2425 690 \nQ 2706 997 2706 1503 \nQ 2706 1984 2436 2262 \nQ 2166 2541 1728 2541 \nQ 1456 2541 1237 2417 \nQ 1019 2294 894 2097 \nL 366 2166 \nL 809 4519 \nL 3088 4519 \nL 3088 3981 \nL 1259 3981 \nL 1013 2750 \nQ 1425 3038 1878 3038 \nQ 2478 3038 2890 2622 \nQ 3303 2206 3303 1553 \nQ 3303 931 2941 478 \nQ 2500 -78 1738 -78 \nQ 1113 -78 717 272 \nQ 322 622 266 1200 \nz\n\" id=\"ArialMT-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-35\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 451.712754 537.78 \nL 451.712754 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g style=\"fill:#262626;\" transform=\"translate(445.595723 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 3184 3459 \nL 2625 3416 \nQ 2550 3747 2413 3897 \nQ 2184 4138 1850 4138 \nQ 1581 4138 1378 3988 \nQ 1113 3794 959 3422 \nQ 806 3050 800 2363 \nQ 1003 2672 1297 2822 \nQ 1591 2972 1913 2972 \nQ 2475 2972 2870 2558 \nQ 3266 2144 3266 1488 \nQ 3266 1056 3080 686 \nQ 2894 316 2569 119 \nQ 2244 -78 1831 -78 \nQ 1128 -78 684 439 \nQ 241 956 241 2144 \nQ 241 3472 731 4075 \nQ 1159 4600 1884 4600 \nQ 2425 4600 2770 4297 \nQ 3116 3994 3184 3459 \nz\nM 888 1484 \nQ 888 1194 1011 928 \nQ 1134 663 1356 523 \nQ 1578 384 1822 384 \nQ 2178 384 2434 671 \nQ 2691 959 2691 1453 \nQ 2691 1928 2437 2201 \nQ 2184 2475 1800 2475 \nQ 1419 2475 1153 2201 \nQ 888 1928 888 1484 \nz\n\" id=\"ArialMT-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-36\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 515.880052 537.78 \nL 515.880052 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 70 -->\n      <g style=\"fill:#262626;\" transform=\"translate(509.763021 555.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 303 3981 \nL 303 4522 \nL 3269 4522 \nL 3269 4084 \nQ 2831 3619 2401 2847 \nQ 1972 2075 1738 1259 \nQ 1569 684 1522 0 \nL 944 0 \nQ 953 541 1156 1306 \nQ 1359 2072 1739 2783 \nQ 2119 3494 2547 3981 \nL 303 3981 \nz\n\" id=\"ArialMT-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-37\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 537.78 \nL 558.230469 537.78 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.408438 541.716797)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 446.568035 \nL 558.230469 446.568035 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(28.291406 450.504832)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-35\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 355.35607 \nL 558.230469 355.35607 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 359.292867)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 264.144105 \nL 558.230469 264.144105 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 150 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 268.080902)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 172.93214 \nL 558.230469 172.93214 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 176.868937)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 50.025469 81.720175 \nL 558.230469 81.720175 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 250 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 85.656972)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Count -->\n     <g style=\"fill:#262626;\" transform=\"translate(15.789375 288.499687)rotate(-90)scale(0.12 -0.12)\">\n      <defs>\n       <path d=\"M 3763 1606 \nL 4369 1453 \nQ 4178 706 3683 314 \nQ 3188 -78 2472 -78 \nQ 1731 -78 1267 223 \nQ 803 525 561 1097 \nQ 319 1669 319 2325 \nQ 319 3041 592 3573 \nQ 866 4106 1370 4382 \nQ 1875 4659 2481 4659 \nQ 3169 4659 3637 4309 \nQ 4106 3959 4291 3325 \nL 3694 3184 \nQ 3534 3684 3231 3912 \nQ 2928 4141 2469 4141 \nQ 1941 4141 1586 3887 \nQ 1231 3634 1087 3207 \nQ 944 2781 944 2328 \nQ 944 1744 1114 1308 \nQ 1284 872 1643 656 \nQ 2003 441 2422 441 \nQ 2931 441 3284 734 \nQ 3638 1028 3763 1606 \nz\n\" id=\"ArialMT-43\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 213 1659 \nQ 213 2581 725 3025 \nQ 1153 3394 1769 3394 \nQ 2453 3394 2887 2945 \nQ 3322 2497 3322 1706 \nQ 3322 1066 3130 698 \nQ 2938 331 2570 128 \nQ 2203 -75 1769 -75 \nQ 1072 -75 642 372 \nQ 213 819 213 1659 \nz\nM 791 1659 \nQ 791 1022 1069 705 \nQ 1347 388 1769 388 \nQ 2188 388 2466 706 \nQ 2744 1025 2744 1678 \nQ 2744 2294 2464 2611 \nQ 2184 2928 1769 2928 \nQ 1347 2928 1069 2612 \nQ 791 2297 791 1659 \nz\n\" id=\"ArialMT-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2597 0 \nL 2597 488 \nQ 2209 -75 1544 -75 \nQ 1250 -75 995 37 \nQ 741 150 617 320 \nQ 494 491 444 738 \nQ 409 903 409 1263 \nL 409 3319 \nL 972 3319 \nL 972 1478 \nQ 972 1038 1006 884 \nQ 1059 663 1231 536 \nQ 1403 409 1656 409 \nQ 1909 409 2131 539 \nQ 2353 669 2445 892 \nQ 2538 1116 2538 1541 \nL 2538 3319 \nL 3100 3319 \nL 3100 0 \nL 2597 0 \nz\n\" id=\"ArialMT-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 422 0 \nL 422 3319 \nL 928 3319 \nL 928 2847 \nQ 1294 3394 1984 3394 \nQ 2284 3394 2536 3286 \nQ 2788 3178 2913 3003 \nQ 3038 2828 3088 2588 \nQ 3119 2431 3119 2041 \nL 3119 0 \nL 2556 0 \nL 2556 2019 \nQ 2556 2363 2490 2533 \nQ 2425 2703 2258 2804 \nQ 2091 2906 1866 2906 \nQ 1506 2906 1245 2678 \nQ 984 2450 984 1813 \nL 984 0 \nL 422 0 \nz\n\" id=\"ArialMT-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1650 503 \nL 1731 6 \nQ 1494 -44 1306 -44 \nQ 1000 -44 831 53 \nQ 663 150 594 308 \nQ 525 466 525 972 \nL 525 2881 \nL 113 2881 \nL 113 3319 \nL 525 3319 \nL 525 4141 \nL 1084 4478 \nL 1084 3319 \nL 1650 3319 \nL 1650 2881 \nL 1084 2881 \nL 1084 941 \nQ 1084 700 1114 631 \nQ 1144 563 1211 522 \nQ 1278 481 1403 481 \nQ 1497 481 1650 503 \nz\n\" id=\"ArialMT-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-43\"/>\n      <use x=\"72.216797\" xlink:href=\"#ArialMT-6f\"/>\n      <use x=\"127.832031\" xlink:href=\"#ArialMT-75\"/>\n      <use x=\"183.447266\" xlink:href=\"#ArialMT-6e\"/>\n      <use x=\"239.0625\" xlink:href=\"#ArialMT-74\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 73.125696 537.78 \nL 85.28371 537.78 \nL 85.28371 535.955761 \nL 73.125696 535.955761 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 85.28371 537.78 \nL 97.441725 537.78 \nL 97.441725 523.186086 \nL 85.28371 523.186086 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 97.441725 537.78 \nL 109.599739 537.78 \nL 109.599739 468.458907 \nL 97.441725 468.458907 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 109.599739 537.78 \nL 121.757753 537.78 \nL 121.757753 446.568035 \nL 109.599739 446.568035 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 121.757753 537.78 \nL 133.915768 537.78 \nL 133.915768 417.380206 \nL 121.757753 417.380206 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 133.915768 537.78 \nL 146.073782 537.78 \nL 146.073782 375.422702 \nL 133.915768 375.422702 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 146.073782 537.78 \nL 158.231797 537.78 \nL 158.231797 342.586395 \nL 146.073782 342.586395 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 158.231797 537.78 \nL 170.389811 537.78 \nL 170.389811 258.671387 \nL 158.231797 258.671387 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 170.389811 537.78 \nL 182.547825 537.78 \nL 182.547825 251.37443 \nL 170.389811 251.37443 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 182.547825 537.78 \nL 194.70584 537.78 \nL 194.70584 375.422702 \nL 182.547825 375.422702 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 194.70584 537.78 \nL 206.863854 537.78 \nL 206.863854 163.810944 \nL 194.70584 163.810944 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 206.863854 537.78 \nL 219.021868 537.78 \nL 219.021868 107.259526 \nL 206.863854 107.259526 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 219.021868 537.78 \nL 231.179883 537.78 \nL 231.179883 90.841372 \nL 219.021868 90.841372 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 231.179883 537.78 \nL 243.337897 537.78 \nL 243.337897 70.77474 \nL 231.179883 70.77474 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 243.337897 537.78 \nL 255.495911 537.78 \nL 255.495911 32.465714 \nL 243.337897 32.465714 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 255.495911 537.78 \nL 267.653926 537.78 \nL 267.653926 74.423218 \nL 255.495911 74.423218 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 267.653926 537.78 \nL 279.81194 537.78 \nL 279.81194 52.532347 \nL 267.653926 52.532347 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 279.81194 537.78 \nL 291.969954 537.78 \nL 291.969954 78.071697 \nL 279.81194 78.071697 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 291.969954 537.78 \nL 304.127969 537.78 \nL 304.127969 346.234874 \nL 291.969954 346.234874 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 304.127969 537.78 \nL 316.285983 537.78 \nL 316.285983 94.48985 \nL 304.127969 94.48985 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 316.285983 537.78 \nL 328.443997 537.78 \nL 328.443997 202.119969 \nL 316.285983 202.119969 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 328.443997 537.78 \nL 340.602012 537.78 \nL 340.602012 192.998773 \nL 328.443997 192.998773 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 340.602012 537.78 \nL 352.760026 537.78 \nL 352.760026 307.925848 \nL 340.602012 307.925848 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 352.760026 537.78 \nL 364.918041 537.78 \nL 364.918041 380.89542 \nL 352.760026 380.89542 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 364.918041 537.78 \nL 377.076055 537.78 \nL 377.076055 437.446839 \nL 364.918041 437.446839 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 377.076055 537.78 \nL 389.234069 537.78 \nL 389.234069 472.107385 \nL 377.076055 472.107385 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 389.234069 537.78 \nL 401.392084 537.78 \nL 401.392084 486.7013 \nL 389.234069 486.7013 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 401.392084 537.78 \nL 413.550098 537.78 \nL 413.550098 493.998257 \nL 401.392084 493.998257 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 413.550098 537.78 \nL 425.708112 537.78 \nL 425.708112 517.713368 \nL 413.550098 517.713368 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 425.708112 537.78 \nL 437.866127 537.78 \nL 437.866127 519.537607 \nL 425.708112 519.537607 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 437.866127 537.78 \nL 450.024141 537.78 \nL 450.024141 521.361846 \nL 437.866127 521.361846 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 450.024141 537.78 \nL 462.182155 537.78 \nL 462.182155 530.483043 \nL 450.024141 530.483043 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 462.182155 537.78 \nL 474.34017 537.78 \nL 474.34017 534.131521 \nL 462.182155 534.131521 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 474.34017 537.78 \nL 486.498184 537.78 \nL 486.498184 526.834564 \nL 474.34017 526.834564 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 486.498184 537.78 \nL 498.656198 537.78 \nL 498.656198 535.955761 \nL 486.498184 535.955761 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 498.656198 537.78 \nL 510.814213 537.78 \nL 510.814213 535.955761 \nL 498.656198 535.955761 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 510.814213 537.78 \nL 522.972227 537.78 \nL 522.972227 537.78 \nL 510.814213 537.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 522.972227 537.78 \nL 535.130241 537.78 \nL 535.130241 532.307282 \nL 522.972227 532.307282 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pb0eac8cc4c)\" d=\"M 73.125696 531.717635 \nL 75.447327 529.924328 \nL 77.768958 527.737496 \nL 80.090589 525.122547 \nL 82.41222 522.058512 \nL 84.73385 518.541647 \nL 87.055481 514.587432 \nL 89.377112 510.230371 \nL 91.698743 505.52138 \nL 96.342005 495.30278 \nL 103.306897 478.924845 \nL 114.915052 450.999422 \nL 121.879945 433.702855 \nL 128.844837 415.605146 \nL 133.488099 402.970656 \nL 138.131361 389.678698 \nL 142.774622 375.448451 \nL 147.417884 360.06252 \nL 152.061146 343.602977 \nL 161.34767 309.840088 \nL 165.990931 294.131439 \nL 170.634193 279.757474 \nL 177.599086 259.751514 \nL 182.242347 246.118891 \nL 184.563978 238.872867 \nL 186.885609 231.247565 \nL 191.528871 214.823973 \nL 196.172133 197.191337 \nL 203.137025 170.452571 \nL 205.458656 162.007573 \nL 207.780287 153.998228 \nL 210.101918 146.507863 \nL 212.423549 139.584479 \nL 214.74518 133.237738 \nL 217.066811 127.440196 \nL 219.388442 122.133186 \nL 221.710072 117.237202 \nL 224.031703 112.665756 \nL 228.674965 104.20779 \nL 233.318227 96.473458 \nL 235.639858 92.946062 \nL 237.961489 89.746012 \nL 240.28312 86.96731 \nL 242.60475 84.697313 \nL 244.926381 82.999362 \nL 247.248012 81.899904 \nL 249.569643 81.382755 \nL 251.891274 81.39163 \nL 254.212905 81.840205 \nL 256.534536 82.62738 \nL 258.856167 83.654365 \nL 263.499428 86.131296 \nL 268.14269 88.978142 \nL 270.464321 90.574034 \nL 272.785952 92.337563 \nL 275.107583 94.310819 \nL 277.429214 96.526511 \nL 279.750844 99.0026 \nL 282.072475 101.741508 \nL 284.394106 104.733706 \nL 286.715737 107.964554 \nL 289.037368 111.422716 \nL 291.358999 115.108213 \nL 293.68063 119.038227 \nL 296.002261 123.249208 \nL 298.323892 127.794534 \nL 300.645522 132.737914 \nL 302.967153 138.143695 \nL 305.288784 144.066091 \nL 307.610415 150.539789 \nL 309.932046 157.574247 \nL 312.253677 165.153203 \nL 314.575308 173.239672 \nL 319.218569 190.740918 \nL 323.861831 209.741576 \nL 328.505093 230.113004 \nL 333.148355 251.873731 \nL 337.791617 274.916294 \nL 354.043033 357.484679 \nL 358.686294 378.9732 \nL 363.329556 398.876216 \nL 367.972818 417.073426 \nL 370.294449 425.496704 \nL 372.61608 433.44764 \nL 374.937711 440.908733 \nL 377.259342 447.863762 \nL 379.580972 454.299784 \nL 381.902603 460.20901 \nL 384.224234 465.590325 \nL 386.545865 470.450365 \nL 388.867496 474.804154 \nL 391.189127 478.675389 \nL 393.510758 482.096445 \nL 395.832389 485.108162 \nL 398.154019 487.759359 \nL 400.47565 490.105964 \nL 402.797281 492.209568 \nL 407.440543 495.948281 \nL 414.405436 501.287433 \nL 419.048697 505.132223 \nL 432.978483 517.251725 \nL 435.300114 518.996254 \nL 437.621744 520.595543 \nL 439.943375 522.048218 \nL 444.586637 524.549445 \nL 449.229899 526.599648 \nL 453.873161 528.270401 \nL 458.516422 529.564222 \nL 463.159684 530.462798 \nL 467.802946 531.009222 \nL 481.732731 532.172996 \nL 486.375993 532.885273 \nL 500.305778 535.287552 \nL 504.94904 535.741668 \nL 509.592302 535.941186 \nL 514.235564 535.933823 \nL 530.48698 535.640493 \nL 535.130241 535.829385 \nL 535.130241 535.829385 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 50.025469 537.78 \nL 50.025469 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path d=\"M 50.025469 537.78 \nL 558.230469 537.78 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb0eac8cc4c\">\n   <rect height=\"530.58\" width=\"508.205\" x=\"50.025469\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "np.unique(dataset['star_rating'])\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"636.539844pt\" version=\"1.1\" viewBox=\"0 0 637.305469 636.539844\" width=\"637.305469pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-30T18:50:59.928427</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 636.539844 \nL 637.305469 636.539844 \nL 637.305469 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.025469 609.78 \nL 630.105469 609.78 \nL 630.105469 7.2 \nL 50.025469 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 76.392741 609.78 \nL 76.392741 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(73.334226 627.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 266 2259 \nQ 266 3072 433 3567 \nQ 600 4063 929 4331 \nQ 1259 4600 1759 4600 \nQ 2128 4600 2406 4451 \nQ 2684 4303 2865 4023 \nQ 3047 3744 3150 3342 \nQ 3253 2941 3253 2259 \nQ 3253 1453 3087 958 \nQ 2922 463 2592 192 \nQ 2263 -78 1759 -78 \nQ 1097 -78 719 397 \nQ 266 969 266 2259 \nz\nM 844 2259 \nQ 844 1131 1108 757 \nQ 1372 384 1759 384 \nQ 2147 384 2411 759 \nQ 2675 1134 2675 2259 \nQ 2675 3391 2411 3762 \nQ 2147 4134 1753 4134 \nQ 1366 4134 1134 3806 \nQ 844 3388 844 2259 \nz\n\" id=\"ArialMT-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 191.408108 609.78 \nL 191.408108 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1000 -->\n      <g style=\"fill:#262626;\" transform=\"translate(179.174045 627.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 2384 0 \nL 1822 0 \nL 1822 3584 \nQ 1619 3391 1289 3197 \nQ 959 3003 697 2906 \nL 697 3450 \nQ 1169 3672 1522 3987 \nQ 1875 4303 2022 4600 \nL 2384 4600 \nL 2384 0 \nz\n\" id=\"ArialMT-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 306.423474 609.78 \nL 306.423474 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2000 -->\n      <g style=\"fill:#262626;\" transform=\"translate(294.189412 627.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 3222 541 \nL 3222 0 \nL 194 0 \nQ 188 203 259 391 \nQ 375 700 629 1000 \nQ 884 1300 1366 1694 \nQ 2113 2306 2375 2664 \nQ 2638 3022 2638 3341 \nQ 2638 3675 2398 3904 \nQ 2159 4134 1775 4134 \nQ 1369 4134 1125 3890 \nQ 881 3647 878 3216 \nL 300 3275 \nQ 359 3922 746 4261 \nQ 1134 4600 1788 4600 \nQ 2447 4600 2831 4234 \nQ 3216 3869 3216 3328 \nQ 3216 3053 3103 2787 \nQ 2991 2522 2730 2228 \nQ 2469 1934 1863 1422 \nQ 1356 997 1212 845 \nQ 1069 694 975 541 \nL 3222 541 \nz\n\" id=\"ArialMT-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 421.43884 609.78 \nL 421.43884 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3000 -->\n      <g style=\"fill:#262626;\" transform=\"translate(409.204778 627.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 269 1209 \nL 831 1284 \nQ 928 806 1161 595 \nQ 1394 384 1728 384 \nQ 2125 384 2398 659 \nQ 2672 934 2672 1341 \nQ 2672 1728 2419 1979 \nQ 2166 2231 1775 2231 \nQ 1616 2231 1378 2169 \nL 1441 2663 \nQ 1497 2656 1531 2656 \nQ 1891 2656 2178 2843 \nQ 2466 3031 2466 3422 \nQ 2466 3731 2256 3934 \nQ 2047 4138 1716 4138 \nQ 1388 4138 1169 3931 \nQ 950 3725 888 3313 \nL 325 3413 \nQ 428 3978 793 4289 \nQ 1159 4600 1703 4600 \nQ 2078 4600 2393 4439 \nQ 2709 4278 2876 4000 \nQ 3044 3722 3044 3409 \nQ 3044 3113 2884 2869 \nQ 2725 2625 2413 2481 \nQ 2819 2388 3044 2092 \nQ 3269 1797 3269 1353 \nQ 3269 753 2831 336 \nQ 2394 -81 1725 -81 \nQ 1122 -81 723 278 \nQ 325 638 269 1209 \nz\n\" id=\"ArialMT-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-33\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 536.454207 609.78 \nL 536.454207 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4000 -->\n      <g style=\"fill:#262626;\" transform=\"translate(524.220144 627.153594)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 2069 0 \nL 2069 1097 \nL 81 1097 \nL 81 1613 \nL 2172 4581 \nL 2631 4581 \nL 2631 1613 \nL 3250 1613 \nL 3250 1097 \nL 2631 1097 \nL 2631 0 \nL 2069 0 \nz\nM 2069 1613 \nL 2069 3678 \nL 634 1613 \nL 2069 1613 \nz\n\" id=\"ArialMT-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-34\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 609.78 \nL 630.105469 609.78 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.408438 613.716797)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 537.319683 \nL 630.105469 537.319683 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(28.291406 541.256479)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 266 1200 \nL 856 1250 \nQ 922 819 1161 601 \nQ 1400 384 1738 384 \nQ 2144 384 2425 690 \nQ 2706 997 2706 1503 \nQ 2706 1984 2436 2262 \nQ 2166 2541 1728 2541 \nQ 1456 2541 1237 2417 \nQ 1019 2294 894 2097 \nL 366 2166 \nL 809 4519 \nL 3088 4519 \nL 3088 3981 \nL 1259 3981 \nL 1013 2750 \nQ 1425 3038 1878 3038 \nQ 2478 3038 2890 2622 \nQ 3303 2206 3303 1553 \nQ 3303 931 2941 478 \nQ 2500 -78 1738 -78 \nQ 1113 -78 717 272 \nQ 322 622 266 1200 \nz\n\" id=\"ArialMT-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-35\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 464.859365 \nL 630.105469 464.859365 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 468.796162)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 392.399048 \nL 630.105469 392.399048 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 150 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 396.335844)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 319.93873 \nL 630.105469 319.93873 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 323.875527)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 247.478413 \nL 630.105469 247.478413 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 250 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 251.41521)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 175.018095 \nL 630.105469 175.018095 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 300 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 178.954892)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-33\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 102.557778 \nL 630.105469 102.557778 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 350 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 106.494575)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-33\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p889da5cbe5)\" d=\"M 50.025469 30.09746 \nL 630.105469 30.09746 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 400 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.174375 34.034257)scale(0.11 -0.11)\">\n       <use xlink:href=\"#ArialMT-34\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Count -->\n     <g style=\"fill:#262626;\" transform=\"translate(15.789375 324.499687)rotate(-90)scale(0.12 -0.12)\">\n      <defs>\n       <path d=\"M 3763 1606 \nL 4369 1453 \nQ 4178 706 3683 314 \nQ 3188 -78 2472 -78 \nQ 1731 -78 1267 223 \nQ 803 525 561 1097 \nQ 319 1669 319 2325 \nQ 319 3041 592 3573 \nQ 866 4106 1370 4382 \nQ 1875 4659 2481 4659 \nQ 3169 4659 3637 4309 \nQ 4106 3959 4291 3325 \nL 3694 3184 \nQ 3534 3684 3231 3912 \nQ 2928 4141 2469 4141 \nQ 1941 4141 1586 3887 \nQ 1231 3634 1087 3207 \nQ 944 2781 944 2328 \nQ 944 1744 1114 1308 \nQ 1284 872 1643 656 \nQ 2003 441 2422 441 \nQ 2931 441 3284 734 \nQ 3638 1028 3763 1606 \nz\n\" id=\"ArialMT-43\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 213 1659 \nQ 213 2581 725 3025 \nQ 1153 3394 1769 3394 \nQ 2453 3394 2887 2945 \nQ 3322 2497 3322 1706 \nQ 3322 1066 3130 698 \nQ 2938 331 2570 128 \nQ 2203 -75 1769 -75 \nQ 1072 -75 642 372 \nQ 213 819 213 1659 \nz\nM 791 1659 \nQ 791 1022 1069 705 \nQ 1347 388 1769 388 \nQ 2188 388 2466 706 \nQ 2744 1025 2744 1678 \nQ 2744 2294 2464 2611 \nQ 2184 2928 1769 2928 \nQ 1347 2928 1069 2612 \nQ 791 2297 791 1659 \nz\n\" id=\"ArialMT-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2597 0 \nL 2597 488 \nQ 2209 -75 1544 -75 \nQ 1250 -75 995 37 \nQ 741 150 617 320 \nQ 494 491 444 738 \nQ 409 903 409 1263 \nL 409 3319 \nL 972 3319 \nL 972 1478 \nQ 972 1038 1006 884 \nQ 1059 663 1231 536 \nQ 1403 409 1656 409 \nQ 1909 409 2131 539 \nQ 2353 669 2445 892 \nQ 2538 1116 2538 1541 \nL 2538 3319 \nL 3100 3319 \nL 3100 0 \nL 2597 0 \nz\n\" id=\"ArialMT-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 422 0 \nL 422 3319 \nL 928 3319 \nL 928 2847 \nQ 1294 3394 1984 3394 \nQ 2284 3394 2536 3286 \nQ 2788 3178 2913 3003 \nQ 3038 2828 3088 2588 \nQ 3119 2431 3119 2041 \nL 3119 0 \nL 2556 0 \nL 2556 2019 \nQ 2556 2363 2490 2533 \nQ 2425 2703 2258 2804 \nQ 2091 2906 1866 2906 \nQ 1506 2906 1245 2678 \nQ 984 2450 984 1813 \nL 984 0 \nL 422 0 \nz\n\" id=\"ArialMT-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1650 503 \nL 1731 6 \nQ 1494 -44 1306 -44 \nQ 1000 -44 831 53 \nQ 663 150 594 308 \nQ 525 466 525 972 \nL 525 2881 \nL 113 2881 \nL 113 3319 \nL 525 3319 \nL 525 4141 \nL 1084 4478 \nL 1084 3319 \nL 1650 3319 \nL 1650 2881 \nL 1084 2881 \nL 1084 941 \nQ 1084 700 1114 631 \nQ 1144 563 1211 522 \nQ 1278 481 1403 481 \nQ 1497 481 1650 503 \nz\n\" id=\"ArialMT-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-43\"/>\n      <use x=\"72.216797\" xlink:href=\"#ArialMT-6f\"/>\n      <use x=\"127.832031\" xlink:href=\"#ArialMT-75\"/>\n      <use x=\"183.447266\" xlink:href=\"#ArialMT-6e\"/>\n      <use x=\"239.0625\" xlink:href=\"#ArialMT-74\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 76.392741 609.78 \nL 80.545068 609.78 \nL 80.545068 608.330794 \nL 76.392741 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 80.545068 609.78 \nL 84.697394 609.78 \nL 84.697394 608.330794 \nL 80.545068 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 84.697394 609.78 \nL 88.849721 609.78 \nL 88.849721 602.533968 \nL 84.697394 602.533968 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 88.849721 609.78 \nL 93.002047 609.78 \nL 93.002047 551.811746 \nL 88.849721 551.811746 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 93.002047 609.78 \nL 97.154374 609.78 \nL 97.154374 456.164127 \nL 93.002047 456.164127 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 97.154374 609.78 \nL 101.3067 609.78 \nL 101.3067 372.110159 \nL 97.154374 372.110159 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 101.3067 609.78 \nL 105.459026 609.78 \nL 105.459026 299.649841 \nL 101.3067 299.649841 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 105.459026 609.78 \nL 109.611353 609.78 \nL 109.611353 208.349841 \nL 105.459026 208.349841 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 109.611353 609.78 \nL 113.763679 609.78 \nL 113.763679 35.894286 \nL 109.611353 35.894286 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 113.763679 609.78 \nL 117.916006 609.78 \nL 117.916006 93.86254 \nL 113.763679 93.86254 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 117.916006 609.78 \nL 122.068332 609.78 \nL 122.068332 177.916508 \nL 117.916006 177.916508 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 122.068332 609.78 \nL 126.220658 609.78 \nL 126.220658 244.58 \nL 122.068332 244.58 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 126.220658 609.78 \nL 130.372985 609.78 \nL 130.372985 240.232381 \nL 126.220658 240.232381 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 130.372985 609.78 \nL 134.525311 609.78 \nL 134.525311 331.532381 \nL 130.372985 331.532381 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 134.525311 609.78 \nL 138.677638 609.78 \nL 138.677638 372.110159 \nL 134.525311 372.110159 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 138.677638 609.78 \nL 142.829964 609.78 \nL 142.829964 382.254603 \nL 138.677638 382.254603 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 142.829964 609.78 \nL 146.982291 609.78 \nL 146.982291 412.687937 \nL 142.829964 412.687937 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 146.982291 609.78 \nL 151.134617 609.78 \nL 151.134617 454.714921 \nL 146.982291 454.714921 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 151.134617 609.78 \nL 155.286943 609.78 \nL 155.286943 477.902222 \nL 151.134617 477.902222 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 155.286943 609.78 \nL 159.43927 609.78 \nL 159.43927 466.308571 \nL 155.286943 466.308571 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 159.43927 609.78 \nL 163.591596 609.78 \nL 163.591596 490.945079 \nL 159.43927 490.945079 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 163.591596 609.78 \nL 167.743923 609.78 \nL 167.743923 508.335556 \nL 163.591596 508.335556 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 167.743923 609.78 \nL 171.896249 609.78 \nL 171.896249 522.827619 \nL 167.743923 522.827619 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 171.896249 609.78 \nL 176.048575 609.78 \nL 176.048575 532.972063 \nL 171.896249 532.972063 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 176.048575 609.78 \nL 180.200902 609.78 \nL 180.200902 551.811746 \nL 176.048575 551.811746 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 180.200902 609.78 \nL 184.353228 609.78 \nL 184.353228 561.95619 \nL 180.200902 561.95619 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 184.353228 609.78 \nL 188.505555 609.78 \nL 188.505555 561.95619 \nL 184.353228 561.95619 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 188.505555 609.78 \nL 192.657881 609.78 \nL 192.657881 583.694286 \nL 188.505555 583.694286 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 192.657881 609.78 \nL 196.810207 609.78 \nL 196.810207 579.346667 \nL 192.657881 579.346667 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 196.810207 609.78 \nL 200.962534 609.78 \nL 200.962534 577.89746 \nL 196.810207 577.89746 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 200.962534 609.78 \nL 205.11486 609.78 \nL 205.11486 595.287937 \nL 200.962534 595.287937 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 205.11486 609.78 \nL 209.267187 609.78 \nL 209.267187 588.041905 \nL 205.11486 588.041905 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 209.267187 609.78 \nL 213.419513 609.78 \nL 213.419513 598.186349 \nL 209.267187 598.186349 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 213.419513 609.78 \nL 217.57184 609.78 \nL 217.57184 593.83873 \nL 213.419513 593.83873 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 217.57184 609.78 \nL 221.724166 609.78 \nL 221.724166 602.533968 \nL 217.57184 602.533968 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 221.724166 609.78 \nL 225.876492 609.78 \nL 225.876492 586.592698 \nL 221.724166 586.592698 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 225.876492 609.78 \nL 230.028819 609.78 \nL 230.028819 596.737143 \nL 225.876492 596.737143 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 230.028819 609.78 \nL 234.181145 609.78 \nL 234.181145 601.084762 \nL 230.028819 601.084762 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 234.181145 609.78 \nL 238.333472 609.78 \nL 238.333472 606.881587 \nL 234.181145 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 238.333472 609.78 \nL 242.485798 609.78 \nL 242.485798 605.432381 \nL 238.333472 605.432381 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 242.485798 609.78 \nL 246.638124 609.78 \nL 246.638124 602.533968 \nL 242.485798 602.533968 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 246.638124 609.78 \nL 250.790451 609.78 \nL 250.790451 606.881587 \nL 246.638124 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 250.790451 609.78 \nL 254.942777 609.78 \nL 254.942777 603.983175 \nL 250.790451 603.983175 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 254.942777 609.78 \nL 259.095104 609.78 \nL 259.095104 608.330794 \nL 254.942777 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_47\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 259.095104 609.78 \nL 263.24743 609.78 \nL 263.24743 608.330794 \nL 259.095104 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 263.24743 609.78 \nL 267.399757 609.78 \nL 267.399757 603.983175 \nL 263.24743 603.983175 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 267.399757 609.78 \nL 271.552083 609.78 \nL 271.552083 606.881587 \nL 267.399757 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 271.552083 609.78 \nL 275.704409 609.78 \nL 275.704409 609.78 \nL 271.552083 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 275.704409 609.78 \nL 279.856736 609.78 \nL 279.856736 605.432381 \nL 275.704409 605.432381 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_52\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 279.856736 609.78 \nL 284.009062 609.78 \nL 284.009062 608.330794 \nL 279.856736 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_53\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 284.009062 609.78 \nL 288.161389 609.78 \nL 288.161389 609.78 \nL 284.009062 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_54\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 288.161389 609.78 \nL 292.313715 609.78 \nL 292.313715 608.330794 \nL 288.161389 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_55\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 292.313715 609.78 \nL 296.466041 609.78 \nL 296.466041 608.330794 \nL 292.313715 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_56\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 296.466041 609.78 \nL 300.618368 609.78 \nL 300.618368 605.432381 \nL 296.466041 605.432381 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_57\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 300.618368 609.78 \nL 304.770694 609.78 \nL 304.770694 606.881587 \nL 300.618368 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_58\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 304.770694 609.78 \nL 308.923021 609.78 \nL 308.923021 608.330794 \nL 304.770694 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_59\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 308.923021 609.78 \nL 313.075347 609.78 \nL 313.075347 608.330794 \nL 308.923021 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 313.075347 609.78 \nL 317.227673 609.78 \nL 317.227673 606.881587 \nL 313.075347 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 317.227673 609.78 \nL 321.38 609.78 \nL 321.38 608.330794 \nL 317.227673 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_62\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 321.38 609.78 \nL 325.532326 609.78 \nL 325.532326 609.78 \nL 321.38 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_63\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 325.532326 609.78 \nL 329.684653 609.78 \nL 329.684653 608.330794 \nL 325.532326 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_64\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 329.684653 609.78 \nL 333.836979 609.78 \nL 333.836979 609.78 \nL 329.684653 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_65\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 333.836979 609.78 \nL 337.989306 609.78 \nL 337.989306 609.78 \nL 333.836979 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_66\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 337.989306 609.78 \nL 342.141632 609.78 \nL 342.141632 609.78 \nL 337.989306 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_67\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 342.141632 609.78 \nL 346.293958 609.78 \nL 346.293958 609.78 \nL 342.141632 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_68\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 346.293958 609.78 \nL 350.446285 609.78 \nL 350.446285 608.330794 \nL 346.293958 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_69\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 350.446285 609.78 \nL 354.598611 609.78 \nL 354.598611 608.330794 \nL 350.446285 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_70\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 354.598611 609.78 \nL 358.750938 609.78 \nL 358.750938 609.78 \nL 354.598611 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_71\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 358.750938 609.78 \nL 362.903264 609.78 \nL 362.903264 609.78 \nL 358.750938 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_72\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 362.903264 609.78 \nL 367.05559 609.78 \nL 367.05559 609.78 \nL 362.903264 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_73\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 367.05559 609.78 \nL 371.207917 609.78 \nL 371.207917 609.78 \nL 367.05559 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_74\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 371.207917 609.78 \nL 375.360243 609.78 \nL 375.360243 609.78 \nL 371.207917 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_75\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 375.360243 609.78 \nL 379.51257 609.78 \nL 379.51257 609.78 \nL 375.360243 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_76\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 379.51257 609.78 \nL 383.664896 609.78 \nL 383.664896 609.78 \nL 379.51257 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_77\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 383.664896 609.78 \nL 387.817223 609.78 \nL 387.817223 608.330794 \nL 383.664896 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_78\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 387.817223 609.78 \nL 391.969549 609.78 \nL 391.969549 609.78 \nL 387.817223 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_79\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 391.969549 609.78 \nL 396.121875 609.78 \nL 396.121875 609.78 \nL 391.969549 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_80\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 396.121875 609.78 \nL 400.274202 609.78 \nL 400.274202 609.78 \nL 396.121875 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_81\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 400.274202 609.78 \nL 404.426528 609.78 \nL 404.426528 609.78 \nL 400.274202 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_82\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 404.426528 609.78 \nL 408.578855 609.78 \nL 408.578855 606.881587 \nL 404.426528 606.881587 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_83\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 408.578855 609.78 \nL 412.731181 609.78 \nL 412.731181 609.78 \nL 408.578855 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_84\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 412.731181 609.78 \nL 416.883507 609.78 \nL 416.883507 609.78 \nL 412.731181 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_85\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 416.883507 609.78 \nL 421.035834 609.78 \nL 421.035834 609.78 \nL 416.883507 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_86\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 421.035834 609.78 \nL 425.18816 609.78 \nL 425.18816 609.78 \nL 421.035834 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_87\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 425.18816 609.78 \nL 429.340487 609.78 \nL 429.340487 609.78 \nL 425.18816 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_88\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 429.340487 609.78 \nL 433.492813 609.78 \nL 433.492813 609.78 \nL 429.340487 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_89\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 433.492813 609.78 \nL 437.645139 609.78 \nL 437.645139 609.78 \nL 433.492813 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_90\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 437.645139 609.78 \nL 441.797466 609.78 \nL 441.797466 609.78 \nL 437.645139 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_91\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 441.797466 609.78 \nL 445.949792 609.78 \nL 445.949792 608.330794 \nL 441.797466 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_92\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 445.949792 609.78 \nL 450.102119 609.78 \nL 450.102119 608.330794 \nL 445.949792 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_93\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 450.102119 609.78 \nL 454.254445 609.78 \nL 454.254445 609.78 \nL 450.102119 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_94\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 454.254445 609.78 \nL 458.406772 609.78 \nL 458.406772 609.78 \nL 454.254445 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_95\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 458.406772 609.78 \nL 462.559098 609.78 \nL 462.559098 609.78 \nL 458.406772 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_96\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 462.559098 609.78 \nL 466.711424 609.78 \nL 466.711424 609.78 \nL 462.559098 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_97\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 466.711424 609.78 \nL 470.863751 609.78 \nL 470.863751 609.78 \nL 466.711424 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_98\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 470.863751 609.78 \nL 475.016077 609.78 \nL 475.016077 609.78 \nL 470.863751 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_99\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 475.016077 609.78 \nL 479.168404 609.78 \nL 479.168404 609.78 \nL 475.016077 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_100\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 479.168404 609.78 \nL 483.32073 609.78 \nL 483.32073 609.78 \nL 479.168404 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_101\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 483.32073 609.78 \nL 487.473056 609.78 \nL 487.473056 609.78 \nL 483.32073 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_102\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 487.473056 609.78 \nL 491.625383 609.78 \nL 491.625383 609.78 \nL 487.473056 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_103\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 491.625383 609.78 \nL 495.777709 609.78 \nL 495.777709 609.78 \nL 491.625383 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_104\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 495.777709 609.78 \nL 499.930036 609.78 \nL 499.930036 609.78 \nL 495.777709 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_105\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 499.930036 609.78 \nL 504.082362 609.78 \nL 504.082362 609.78 \nL 499.930036 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_106\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 504.082362 609.78 \nL 508.234689 609.78 \nL 508.234689 609.78 \nL 504.082362 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_107\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 508.234689 609.78 \nL 512.387015 609.78 \nL 512.387015 609.78 \nL 508.234689 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_108\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 512.387015 609.78 \nL 516.539341 609.78 \nL 516.539341 609.78 \nL 512.387015 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_109\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 516.539341 609.78 \nL 520.691668 609.78 \nL 520.691668 609.78 \nL 516.539341 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_110\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 520.691668 609.78 \nL 524.843994 609.78 \nL 524.843994 609.78 \nL 520.691668 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_111\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 524.843994 609.78 \nL 528.996321 609.78 \nL 528.996321 609.78 \nL 524.843994 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_112\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 528.996321 609.78 \nL 533.148647 609.78 \nL 533.148647 609.78 \nL 528.996321 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_113\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 533.148647 609.78 \nL 537.300973 609.78 \nL 537.300973 609.78 \nL 533.148647 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_114\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 537.300973 609.78 \nL 541.4533 609.78 \nL 541.4533 609.78 \nL 537.300973 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_115\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 541.4533 609.78 \nL 545.605626 609.78 \nL 545.605626 609.78 \nL 541.4533 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_116\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 545.605626 609.78 \nL 549.757953 609.78 \nL 549.757953 609.78 \nL 545.605626 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_117\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 549.757953 609.78 \nL 553.910279 609.78 \nL 553.910279 609.78 \nL 549.757953 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_118\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 553.910279 609.78 \nL 558.062605 609.78 \nL 558.062605 609.78 \nL 553.910279 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_119\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 558.062605 609.78 \nL 562.214932 609.78 \nL 562.214932 609.78 \nL 558.062605 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_120\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 562.214932 609.78 \nL 566.367258 609.78 \nL 566.367258 609.78 \nL 562.214932 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_121\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 566.367258 609.78 \nL 570.519585 609.78 \nL 570.519585 609.78 \nL 566.367258 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_122\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 570.519585 609.78 \nL 574.671911 609.78 \nL 574.671911 609.78 \nL 570.519585 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_123\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 574.671911 609.78 \nL 578.824238 609.78 \nL 578.824238 609.78 \nL 574.671911 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_124\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 578.824238 609.78 \nL 582.976564 609.78 \nL 582.976564 609.78 \nL 578.824238 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_125\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 582.976564 609.78 \nL 587.12889 609.78 \nL 587.12889 609.78 \nL 582.976564 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_126\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 587.12889 609.78 \nL 591.281217 609.78 \nL 591.281217 609.78 \nL 587.12889 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_127\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 591.281217 609.78 \nL 595.433543 609.78 \nL 595.433543 609.78 \nL 591.281217 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_128\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 595.433543 609.78 \nL 599.58587 609.78 \nL 599.58587 609.78 \nL 595.433543 609.78 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"patch_129\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 599.58587 609.78 \nL 603.738196 609.78 \nL 603.738196 608.330794 \nL 599.58587 608.330794 \nz\n\" style=\"fill:#4c72b0;fill-opacity:0.5;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.421899;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p889da5cbe5)\" d=\"M 76.392741 607.20933 \nL 79.042719 603.845233 \nL 81.692696 596.881496 \nL 84.342673 584.17172 \nL 86.99265 563.751537 \nL 89.642627 534.635256 \nL 92.292604 497.22978 \nL 94.942582 452.992312 \nL 97.592559 403.623077 \nL 105.54249 242.637601 \nL 108.192467 196.288579 \nL 110.842445 162.984186 \nL 113.492422 147.216815 \nL 116.142399 149.584003 \nL 118.792376 166.589326 \nL 121.442353 192.581193 \nL 129.392285 281.596578 \nL 132.042262 309.057196 \nL 134.692239 334.491171 \nL 137.342216 357.682476 \nL 139.992193 378.802879 \nL 142.64217 398.366185 \nL 145.292148 416.82994 \nL 147.942125 434.188232 \nL 150.592102 449.922905 \nL 153.242079 463.395081 \nL 155.892056 474.408433 \nL 158.542033 483.497762 \nL 163.841988 499.913401 \nL 166.491965 508.616966 \nL 174.441896 535.752315 \nL 177.091874 544.065201 \nL 179.741851 551.653689 \nL 182.391828 558.430197 \nL 185.041805 564.364083 \nL 187.691782 569.459044 \nL 190.341759 573.754308 \nL 192.991736 577.363068 \nL 195.641714 580.487757 \nL 198.291691 583.339976 \nL 200.941668 586.018451 \nL 203.591645 588.482479 \nL 206.241622 590.642302 \nL 208.891599 592.4386 \nL 211.541577 593.829805 \nL 214.191554 594.76256 \nL 216.841531 595.228319 \nL 222.141485 595.550983 \nL 224.791462 596.133874 \nL 227.44144 597.322814 \nL 235.391371 602.36513 \nL 238.041348 603.486252 \nL 240.691325 604.176487 \nL 245.99128 604.916942 \nL 251.291234 605.685097 \nL 256.591188 606.42875 \nL 261.891143 606.736893 \nL 267.191097 606.944119 \nL 275.141028 607.491316 \nL 285.740937 608.335386 \nL 288.390914 608.269686 \nL 293.690868 607.672882 \nL 296.340846 607.34435 \nL 298.990823 607.157155 \nL 301.6408 607.172648 \nL 306.940754 607.640479 \nL 312.240709 608.10849 \nL 336.090503 609.498471 \nL 341.390457 609.486654 \nL 351.990366 609.073228 \nL 357.29032 609.289763 \nL 365.240252 609.689959 \nL 373.190183 609.732908 \nL 402.339932 609.091935 \nL 404.989909 609.018076 \nL 410.289863 609.213176 \nL 418.239795 609.681394 \nL 426.189726 609.762203 \nL 434.139658 609.59653 \nL 444.739567 609.11868 \nL 450.039521 609.220067 \nL 460.639429 609.705589 \nL 471.239338 609.779239 \nL 590.48831 609.737776 \nL 603.738196 609.400245 \nL 603.738196 609.400245 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_130\">\n    <path d=\"M 50.025469 609.78 \nL 50.025469 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_131\">\n    <path d=\"M 50.025469 609.78 \nL 630.105469 609.78 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p889da5cbe5\">\n   <rect height=\"602.58\" width=\"580.08\" x=\"50.025469\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "SEQ_MAX_LEN = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "model_name = 'mrm8488/convbert-base-spanish'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 370/370 [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 242k/242k [00:04<00:00, 48.5kB/s]\n",
      "Downloading: 100%|██████████| 481k/481k [00:07<00:00, 65.9kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 45.7kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "def tokenize_sequence(sequence):\n",
    "    tokens = tokenizer.encode_plus(sequence,\n",
    "                                   max_length=SEQ_MAX_LEN,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   add_special_tokens=True,\n",
    "                                   return_token_type_ids=False,\n",
    "                                   return_attention_mask=True,\n",
    "                                   return_tensors='tf')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenize_sequence('hola, buen dia')\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 50), dtype=int32, numpy=\n",
       "array([[   4, 9050, 1017, 1594, 6076,    5,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "metadata": {},
     "execution_count": 221
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "Xids = np.zeros((len(dataset), SEQ_MAX_LEN))\n",
    "Xmask = np.zeros((len(dataset), SEQ_MAX_LEN))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "for i, seq in enumerate(dataset['review_summary']):\n",
    "    tokens = tokenize_sequence(seq)\n",
    "\n",
    "    Xids[i, :] = tokens['input_ids']\n",
    "    Xmask[i, :] = tokens['attention_mask']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "Xids[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.0000e+00, 1.5820e+03, 1.0170e+03, 1.0670e+03, 2.3680e+03,\n",
       "        1.2000e+03, 1.1530e+03, 2.5570e+03, 1.0640e+03, 5.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "       [4.0000e+00, 1.1840e+03, 1.0447e+04, 1.0360e+03, 1.0300e+03,\n",
       "        6.1290e+03, 1.0080e+03, 1.0490e+03, 9.8200e+02, 1.3480e+03,\n",
       "        9.8100e+02, 1.0947e+04, 3.0931e+04, 5.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "Xmask[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "np.unique(dataset['star_rating'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "label_arr = dataset['star_rating']\n",
    "\n",
    "labels = np.zeros((len(label_arr), np.max(label_arr)))\n",
    "labels.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3872, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 230
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "source": [
    "labels[np.arange(len(label_arr)), np.subtract(label_arr,  1)] = 1\n",
    "\n",
    "labels[:5]\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def serialize_tensors(tensor, name, path):\n",
    "    with open(f'{os.sep.join([path, name])}', 'wb') as file:\n",
    "        np.save(file, tensor)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "!mkdir -p tensors/classifier "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "serialize_tensors(Xids, 'Xids.npy', os.sep.join(\n",
    "    ['.', 'tensors', 'classifier']))\n",
    "serialize_tensors(Xmask, 'Xmask.npy', os.sep.join(\n",
    "    ['.', 'tensors', 'classifier']))\n",
    "serialize_tensors(labels, 'labels.npy', os.sep.join(\n",
    "    ['.', 'tensors', 'classifier']))\n",
    "\n",
    "!ls ./tensors/classifier\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Xids.npy   Xmask.npy  labels.npy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "del dataset, Xids, Xmask, label_arr, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "Xids[:2]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_tensors(name, path):\n",
    "    with open(os.sep.join([path, name]), 'rb') as f:\n",
    "        return np.load(f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "Xids = load_tensors('Xids.npy', os.sep.join(['.', 'tensors', 'classifier']))\n",
    "Xmask = load_tensors('Xmask.npy', os.sep.join(['.', 'tensors', 'classifier']))\n",
    "labels = load_tensors('labels.npy', os.sep.join(\n",
    "    ['.', 'tensors', 'classifier']))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "devices"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "source": [
    "dataset.take(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((50,), (50,), (5,)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "def map_tensor(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "dataset = dataset.map(map_tensor)\n",
    "\n",
    "dataset.take(1)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (50,), attention_mask: (50,)}, (5,)), types: ({input_ids: tf.float64, attention_mask: tf.float64}, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "source": [
    "dataset = dataset.shuffle(10_000).batch(16, drop_remainder=True)\n",
    "\n",
    "dataset.take(1)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (16, 50), attention_mask: (16, 50)}, (16, 5)), types: ({input_ids: tf.float64, attention_mask: tf.float64}, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "source": [
    "DS_LEN = len(list(dataset))\n",
    "DS_LEN\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "SPLIT = 0.85\n",
    "\n",
    "train = dataset.take(round(DS_LEN * SPLIT))\n",
    "val = dataset.skip(round(DS_LEN * SPLIT))\n",
    "\n",
    "del dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "from transformers import TFAutoModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "bert = TFAutoModel.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFConvBertModel.\n",
      "\n",
      "All the layers of TFConvBertModel were initialized from the model checkpoint at mrm8488/convbert-base-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFConvBertModel for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "source": [
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(SEQ_MAX_LEN,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(\n",
    "    shape=(SEQ_MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embedings = bert.convbert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "x = tf.keras.layers.Flatten()(embedings)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "source": [
    "help(bert.convbert)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on TFConvBertMainLayer in module transformers.models.convbert.modeling_tf_convbert object:\n",
      "\n",
      "class TFConvBertMainLayer(keras.engine.base_layer.Layer)\n",
      " |  TFConvBertMainLayer(*args, **kwargs)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TFConvBertMainLayer\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config, **kwargs)\n",
      " |  \n",
      " |  call(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None, training=False, **kwargs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |  \n",
      " |  get_extended_attention_mask(self, attention_mask, input_shape, dtype)\n",
      " |  \n",
      " |  get_head_mask(self, head_mask)\n",
      " |  \n",
      " |  get_input_embeddings(self)\n",
      " |  \n",
      " |  set_input_embeddings(self, value)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  config_class = <class 'transformers.models.convbert.configuration_conv...\n",
      " |      This is the configuration class to store the configuration of a :class:`~transformers.ConvBertModel`. It is used to\n",
      " |      instantiate an ConvBERT model according to the specified arguments, defining the model architecture. Instantiating\n",
      " |      a configuration with the defaults will yield a similar configuration to that of the ConvBERT `conv-bert-base\n",
      " |      <https://huggingface.co/YituTech/conv-bert-base>`__ architecture. Configuration objects inherit from\n",
      " |      :class:`~transformers.PretrainedConfig` and can be used to control the model outputs. Read the documentation from\n",
      " |      :class:`~transformers.PretrainedConfig` for more information.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
      " |              Vocabulary size of the ConvBERT model. Defines the number of different tokens that can be represented by\n",
      " |              the :obj:`inputs_ids` passed when calling :class:`~transformers.ConvBertModel` or\n",
      " |              :class:`~transformers.TFConvBertModel`.\n",
      " |          hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
      " |              Dimensionality of the encoder layers and the pooler layer.\n",
      " |          num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
      " |              Number of hidden layers in the Transformer encoder.\n",
      " |          num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
      " |              Number of attention heads for each attention layer in the Transformer encoder.\n",
      " |          intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
      " |              Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
      " |          hidden_act (:obj:`str` or :obj:`function`, `optional`, defaults to :obj:`\"gelu\"`):\n",
      " |              The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
      " |              :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"selu\"` and :obj:`\"gelu_new\"` are supported.\n",
      " |          hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
      " |              The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
      " |          attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
      " |              The dropout ratio for the attention probabilities.\n",
      " |          max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
      " |              The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
      " |              just in case (e.g., 512 or 1024 or 2048).\n",
      " |          type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
      " |              The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.ConvBertModel`\n",
      " |              or :class:`~transformers.TFConvBertModel`.\n",
      " |          initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
      " |              The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      " |          layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
      " |              The epsilon used by the layer normalization layers.\n",
      " |          head_ratio (:obj:`int`, `optional`, defaults to 2):\n",
      " |              Ratio gamma to reduce the number of attention heads.\n",
      " |          num_groups (:obj:`int`, `optional`, defaults to 1):\n",
      " |              The number of groups for grouped linear layers for ConvBert model\n",
      " |          conv_kernel_size (:obj:`int`, `optional`, defaults to 9):\n",
      " |              The size of the convolutional kernel.\n",
      " |      \n",
      " |      \n",
      " |      Example::\n",
      " |          >>> from transformers import ConvBertModel, ConvBertConfig\n",
      " |          >>> # Initializing a ConvBERT convbert-base-uncased style configuration\n",
      " |          >>> configuration = ConvBertConfig()\n",
      " |          >>> # Initializing a model from the convbert-base-uncased style configuration\n",
      " |          >>> model = ConvBertModel(configuration)\n",
      " |          >>> # Accessing the model configuration\n",
      " |          >>> configuration = model.config\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convbert (TFConvBertMainLayer)  TFBaseModelOutput(la 105680520   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 38400)        0           convbert[6][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          4915328     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 5)            645         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,596,493\n",
      "Trainable params: 4,915,973\n",
      "Non-trainable params: 105,680,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "source": [
    "train.take(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (16, 50), attention_mask: (16, 50)}, (16, 5)), types: ({input_ids: tf.float64, attention_mask: tf.float64}, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 243
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "source": [
    "train.element_spec"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(16, 50), dtype=tf.float64, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(16, 50), dtype=tf.float64, name=None)},\n",
       " TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "source": [
    "train.element_spec == val.element_spec"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 247
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "source": [
    "hist = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=4,\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": " indices[4,34] = 30933 is not in [0, 30522)\n\t [[node model_6/convbert/embeddings/Gather (defined at Users/ernestomancebo/projects/coloquial_bot/env/lib/python3.9/site-packages/transformers/models/convbert/modeling_tf_convbert.py:124) ]] [Op:__inference_train_function_152814]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_6/convbert/embeddings/Gather:\n model_6/Cast (defined at var/folders/6x/pv1k062147x4rdbm_36zxwkr0000gn/T/ipykernel_23729/343053466.py:1)\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6x/pv1k062147x4rdbm_36zxwkr0000gn/T/ipykernel_23729/343053466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/coloquial_bot/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[4,34] = 30933 is not in [0, 30522)\n\t [[node model_6/convbert/embeddings/Gather (defined at Users/ernestomancebo/projects/coloquial_bot/env/lib/python3.9/site-packages/transformers/models/convbert/modeling_tf_convbert.py:124) ]] [Op:__inference_train_function_152814]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_6/convbert/embeddings/Gather:\n model_6/Cast (defined at var/folders/6x/pv1k062147x4rdbm_36zxwkr0000gn/T/ipykernel_23729/343053466.py:1)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('sentiment_roberto')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "300fbada5fe2511d866fde720729699e2c4067aafcaf340c14dab40a90252dcc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}