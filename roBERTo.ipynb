{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# roBERTo\n",
                "This is a BERT based language model which is trained to learn Spanish, since most of the models are in English. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Gathering Dataset\n",
                "\n",
                "As stated, to train the model we need a spanish corpus, therefore we will be using the spanish dataset Dahiana from huggingface.co/"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 142,
            "source": [
                "!pip install datasets"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
                        "Requirement already satisfied: datasets in ./env/lib/python3.9/site-packages (1.11.0)\n",
                        "Requirement already satisfied: packaging in ./env/lib/python3.9/site-packages (from datasets) (21.0)\n",
                        "Requirement already satisfied: dill in ./env/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
                        "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in ./env/lib/python3.9/site-packages (from datasets) (5.0.0)\n",
                        "Requirement already satisfied: tqdm>=4.42 in ./env/lib/python3.9/site-packages (from datasets) (4.62.1)\n",
                        "Requirement already satisfied: multiprocess in ./env/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
                        "Requirement already satisfied: pandas in ./env/lib/python3.9/site-packages (from datasets) (1.3.2)\n",
                        "Requirement already satisfied: xxhash in ./env/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
                        "Requirement already satisfied: huggingface-hub<0.1.0 in ./env/lib/python3.9/site-packages (from datasets) (0.0.12)\n",
                        "Requirement already satisfied: fsspec>=2021.05.0 in ./env/lib/python3.9/site-packages (from datasets) (2021.7.0)\n",
                        "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.9/site-packages (from datasets) (1.19.5)\n",
                        "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
                        "Requirement already satisfied: typing-extensions in ./env/lib/python3.9/site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
                        "Requirement already satisfied: pyparsing>=2.0.2 in ./env/lib/python3.9/site-packages (from packaging->datasets) (2.4.7)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
                        "Requirement already satisfied: charset-normalizer~=2.0.0 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.2)\n",
                        "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
                        "Requirement already satisfied: python-dateutil>=2.7.3 in ./env/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
                        "Requirement already satisfied: pytz>=2017.3 in ./env/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
                        "Requirement already satisfied: six>=1.5 in ./env/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 143,
            "source": [
                "from datasets import load_dataset"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 144,
            "source": [
                "dataset = load_dataset(\"mlsum\", \"es\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Reusing dataset mlsum (/Users/ernestomancebo/.cache/huggingface/datasets/mlsum/es/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 145,
            "source": [
                "dataset"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
                            "        num_rows: 266367\n",
                            "    })\n",
                            "    validation: Dataset({\n",
                            "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
                            "        num_rows: 10358\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
                            "        num_rows: 13920\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 145
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 146,
            "source": [
                "train = dataset['train']\n",
                "validation = dataset['validation']\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 147,
            "source": [
                "train[25]['text']\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'No habrá tregua para el consumidor en 2010. Ni la crisis, ni el paro, ni siquiera el previsible estancamiento de los precios impedirán una subida general de los servicios y suministros más básicos. El afán recaudatorio de las distintas administraciones (Estado, comunidades autónomas y ayuntamientos) para paliar los agujeros de las cuentas públicas han desatado una oleada de aumentos de impuestos, tasas y tarifas de servicios públicos en el año que comienza. El Gobierno está a la cabeza de esta política impositiva. A partir de julio, la práctica totalidad de los productos -exceptuando los de primera necesidad, como el pan- costarán más gracias a la subida de dos puntos del tipo general del IVA, del 16% al 18%. Pensionistas, parados y sueldos bajos mejorarán algo su renta Y si alguien pensaba que el aumento de los impuestos indirectos se va a compensar con una relajación de los directos, los que gravan la renta de cada ciudadano, nada más lejos de la realidad. Los 400 euros de desgravación fiscal, una de las medidas estrella con las que José Luis Rodríguez Zapatero concurrió a las elecciones generales de marzo de 2008, se caen este año del impuesto sobre la renta de las personas físicas (IRPF). Sólo los contribuyentes que apuesten por tener descendencia salen ganando. Se mantiene el cheque bebé de 2.500 euros por hijo. Tampoco les irá mucho mejor a los que decidan ahorrar. Desde el 1 de enero, el recargo para los intereses de cuentas, depósitos y otros productos bancarios también aumenta. En cuanto a los servicios básicos, la palma se la lleva la luz. En enero, la tarifa doméstica -la que pagan la mayor parte de los usuarios- sube un 2,6%. No tiene por qué ser el único aumento en el año, porque las tarifas se revisan trimestralmente, y las compañías eléctricas piden nuevos incrementos. Desde 2006, las tarifas eléctricas han subido muy por encima de la inflación. Las justificaciones son variadas, como los costes del carbón o de las energías renovables, aunque la principal que esgrimen las compañías es el llamado déficit tarifario, es decir, que les cuesta más producir la electricidad de lo que cobran por ella. Esa escasa rentabilidad que arguyen las compañías contrasta con el interés de los inversores por hacerse con ellas como prueba el hecho de que Endesa y Unión Fenosa, la primera y la tercera eléctrica del país, hayan cambiado de manos. El ansia recaudatoria no entiende de colores políticos. Sobre el transporte público hay consenso casi generalizado: sube y mucho. Renfe, controlada por el Gobierno central, socialista, ha decidido un año más aumentar sus precios muy por encima de la inflación, y castigando más a los viajeros de Cercanías (6% de aumento) que a los del AVE (4%). Del otro lado, la Comunidad de Madrid, en manos del Partido Popular, ha impuesto una subida del 21,6% para el billete de 10 viajes, uno de los más utilizados. No todo es negro en el nuevo año. La crisis también ofrece oportunidades. Por ejemplo, es buen momento para comprarse una vivienda. Los precios, que han caído en torno al 7% en 2009, seguirán bajando. Algunos, como el BBVA, opinan que hasta un 20%. Para los que ya compraron piso, pagar la hipoteca les saldrá también más barato. Noviembre marcó el mínimo del Euríbor, el tipo al que están referenciados casi todos los préstamos hipotecarios. Los más desfavorecidos ganan o, por lo menos, no pierden. La mayor parte de las pensiones suben un 1%, y las mínimas una media del 4%. Los que cobren el salario mínimo ven aumentados sus ingresos un 1,5%. Y los parados a los que se les agote el subsidio cobrarán 420 euros durante seis meses más. * Este artículo apareció en la edición impresa del Sábado, 2 de enero de 2010'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 147
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Observamos cuántos registros nos restan si procesamos el corpus de 10 mil en 10 mil entradas"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 148,
            "source": [
                "266367 % 10_000"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "6367"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 148
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 149,
            "source": [
                "import os\n",
                "\n",
                "corpus_dir = os.path.join(os.getcwd(), 'corpus')\n",
                "os.mkdir(corpus_dir)\n",
                "corpus_dir\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "FileExistsError",
                    "evalue": "[Errno 17] File exists: '/Users/ernestomancebo/projects/coloquial_bot/corpus'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
                        "\u001b[0;32m/var/folders/6x/pv1k062147x4rdbm_36zxwkr0000gn/T/ipykernel_7939/4291706709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorpus_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/ernestomancebo/projects/coloquial_bot/corpus'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "\n",
                "def serialize_corpus(dataset, dest_path, max_entries=10_000):\n",
                "    text_data = []\n",
                "    file_count = 0\n",
                "\n",
                "    for sample in tqdm(dataset):\n",
                "        # Clean up a bit the text\n",
                "        text = sample['text']\n",
                "        text = text.replace(\"\\n\", ' ')\n",
                "        text_data.append(text)\n",
                "\n",
                "        if len(text_data) == max_entries:\n",
                "            with open(os.path.join(dest_path, f'es_{file_count}.txt'), 'w', encoding='utf-8') as file:\n",
                "                file.write('\\n'.join(text_data))\n",
                "                text_data = []\n",
                "                file_count += 1\n",
                "\n",
                "                file.close()\n",
                "\n",
                "    # The last iteration may exceed the max_entries cap\n",
                "    if len(text_data) > 0:\n",
                "        with open(os.path.join(dest_path, f'es_{file_count}.txt'), 'w', encoding='utf-8') as file:\n",
                "            file.write('\\n'.join(text_data))\n",
                "\n",
                "            file.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train_dir = os.path.join(corpus_dir, 'train')\n",
                "val_dir = os.path.join(corpus_dir, 'validation')\n",
                "\n",
                "!mkdir {train_dir}\n",
                "!mkdir {val_dir}\n",
                "\n",
                "serialize_corpus(train, train_dir)\n",
                "serialize_corpus(validation, val_dir)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 266367/266367 [01:51<00:00, 2387.99it/s]\n",
                        "100%|██████████| 10358/10358 [00:05<00:00, 1933.35it/s]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Tokenizing"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from pathlib import Path\n",
                "\n",
                "corpus_paths = [str(x) for x in Path('./corpus/train').glob('*.txt')]\n",
                "corpus_paths[:3]\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['corpus/train/es_2.txt', 'corpus/train/es_11.txt', 'corpus/train/es_10.txt']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 69
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tokenizers import ByteLevelBPETokenizer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "VOCAB_SIZE = 32_000\n",
                "\n",
                "tokenizer = ByteLevelBPETokenizer()\n",
                "tokenizer.train(files=corpus_paths,\n",
                "                vocab_size=VOCAB_SIZE,\n",
                "                min_frequency=2,\n",
                "                special_tokens=[\n",
                "                    '<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "!mkdir roberto"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "tokenizer.save_model('roberto')"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['roberto/vocab.json', 'roberto/merges.txt']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 56
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Loading from pretrained model\n",
                "\n",
                "First, we create a model configuration. This config is ok for a small model."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import json\n",
                "config = {\n",
                "    \"attention_probs_dropout_prob\": 0.1,\n",
                "    \"hidden_act\": \"gelu\",\n",
                "    \"hidden_dropout_prob\": 0.3,\n",
                "    \"hidden_size\": 128,\n",
                "    \"initializer_range\": 0.02,\n",
                "    \"num_attention_heads\": 1,\n",
                "    \"num_hidden_layers\": 1,\n",
                "    \"vocab_size\": VOCAB_SIZE,\n",
                "    \"intermediate_size\": 256,\n",
                "    \"max_position_embeddings\": 256\n",
                "}\n",
                "\n",
                "with open(\"./roberto/config.json\", 'w') as fp:\n",
                "    json.dump(config, fp)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from transformers import RobertaTokenizerFast\n",
                "\n",
                "tokenizer = RobertaTokenizerFast.from_pretrained('roberto')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
                        "The class this function is called from is 'RobertaTokenizer'.\n",
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
                        "The class this function is called from is 'RobertaTokenizerFast'.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "TOKEN_MAX_LEN= 512\n",
                "\n",
                "tokenizer('hoy es un buen día', padding='max_length')\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'input_ids': [0, 17756, 317, 298, 1384, 975, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 73
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Input Pipeline and Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import torch\n",
                "\n",
                "\n",
                "def mlm(tensor, percent=0.15):\n",
                "    \"\"\"Mask randomly the given \n",
                "\n",
                "    Args:\n",
                "        tensor ([type]): [description]\n",
                "\n",
                "    Returns:\n",
                "        [type]: [description]\n",
                "    \"\"\"\n",
                "    rand = torch.rand(tensor.shape)  # [0,1]\n",
                "    mask_arr = (rand < 0.15) * (tensor > 2)  # Special tokens: 0, 1, 2\n",
                "\n",
                "    for i in range(tensor.shape[0]):\n",
                "        selection = torch.flatten(mask_arr[i].nonzero())\n",
                "        tensor[i, selection] = 4  # <mask> : 4 in vocab.json\n",
                "\n",
                "    return tensor\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Create three tensors\n",
                "\n",
                "- **Labels**: Are the ground truth of the given input sequence.\n",
                "- **Input Ids**: Are the masked labels, ie. the labels + 0.15% of them masked.\n",
                "- **Attention Maks**\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "input_ids = []\n",
                "masks = []\n",
                "labels = []\n",
                "\n",
                "for path in tqdm(corpus_paths):\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        lines = f.read().split('\\n')\n",
                "    sample = tokenizer(lines,\n",
                "                       max_length=TOKEN_MAX_LEN,\n",
                "                       padding='max_length',\n",
                "                       truncation=True,\n",
                "                       return_tensors='pt')\n",
                "\n",
                "    labels.append(sample.input_ids)\n",
                "    masks.append(sample.attention_mask)\n",
                "    input_ids.append(mlm(sample.input_ids.detach().clone()))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 27/27 [22:25<00:00, 49.83s/it]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Cast the parsed list to tensors"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "input_ids = torch.cat(input_ids)\n",
                "masks = torch.cat(masks)\n",
                "labels = torch.cat(labels)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 178,
            "source": [
                "!mkdir tensors"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 179,
            "source": [
                "torch.save(input_ids, './tensors/input_ids.pt')\n",
                "torch.save(masks, './tensors/masks.pt')\n",
                "torch.save(labels, './tensors/labels.pt')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Loading persisted Tensors"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import torch\n",
                "\n",
                "input_ids = torch.load( './tensors/input_ids.pt')\n",
                "masks = torch.load( './tensors/masks.pt')\n",
                "labels = torch.load( './tensors/labels.pt')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "# Input encoding\n",
                "encodings = {'input_ids': input_ids, 'attention_mask': masks, 'labels': labels}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class Dataset(torch.utils.data.Dataset):\n",
                "\n",
                "    def __init__(self, encodings):\n",
                "        self.encodings = encodings\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.encodings['input_ids'].shape[0]\n",
                "\n",
                "    def __getitem__(self, i):\n",
                "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataset = Dataset(encodings)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from transformers import RobertaConfig\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 170,
            "source": [
                "config = RobertaConfig(\n",
                "    vocab_size=VOCAB_SIZE,\n",
                "    max_position_embeddings=(TOKEN_MAX_LEN + 2),\n",
                "    hidden_size=768,\n",
                "    num_attention_heads=12,\n",
                "    num_hidden_layers=6,\n",
                "    type_vocab_size=1\n",
                ")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from transformers import RobertaForMaskedLM"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 171,
            "source": [
                "model = RobertaForMaskedLM(config)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 172,
            "source": [
                "device = torch.device(\n",
                "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "source": [
                "model.to(device)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "RobertaForMaskedLM(\n",
                            "  (roberta): RobertaModel(\n",
                            "    (embeddings): RobertaEmbeddings(\n",
                            "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
                            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
                            "      (token_type_embeddings): Embedding(1, 768)\n",
                            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "      (dropout): Dropout(p=0.1, inplace=False)\n",
                            "    )\n",
                            "    (encoder): RobertaEncoder(\n",
                            "      (layer): ModuleList(\n",
                            "        (0): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (1): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (2): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (3): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (4): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (5): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "      )\n",
                            "    )\n",
                            "  )\n",
                            "  (lm_head): RobertaLMHead(\n",
                            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "    (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
                            "  )\n",
                            ")"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 173
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from transformers import AdamW"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "source": [
                "model.train() \n",
                "optimizer = AdamW(model.parameters(), lr=1e-4)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "source": [
                "from torch.utils.tensorboard import SummaryWriter\n",
                "\n",
                "writer = SummaryWriter()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 124,
            "source": [
                "from tqdm.auto import tqdm"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 174,
            "source": [
                "\n",
                "epochs = 3\n",
                "step = 0\n",
                "\n",
                "for epoch in range(epochs):\n",
                "\n",
                "    loop = tqdm(dataloader, leave=True)\n",
                "    for batch in loop:\n",
                "        optimizer.zero_grad()\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "\n",
                "        outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
                "        loss = outputs.loss\n",
                "\n",
                "        writer.add_scalar('Loss/Train', loss, epoch)\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        loop.set_description(f'Epoch: {epoch}')\n",
                "        loop.set_postfix(loss=loss.item())\n",
                "\n",
                "model.save('./roberto')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "  0%|          | 0/16648 [00:51<?, ?it/s]\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "AttributeError",
                    "evalue": "'Tensor' object has no attribute 'backwards'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "\u001b[0;32m/var/folders/6x/pv1k062147x4rdbm_36zxwkr0000gn/T/ipykernel_7939/85963333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss/Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'backwards'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## File-Maks testing"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 153,
            "source": [
                "from transformers import pipeline"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "fill = pipeline('fill-mask', model='roberto', tokenizer='roberto')\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('env': venv)"
        },
        "interpreter": {
            "hash": "300fbada5fe2511d866fde720729699e2c4067aafcaf340c14dab40a90252dcc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}